{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils     # Connecting Keypoints Visuals\n",
    "mp_pose = mp.solutions.pose                 # Keypoint detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark Positions\n",
    "1. We will need the angle at the elbow to count the number of curls and also to determine incorrect curl during the exercise. We will figure it out using the coordinates of shoulder, elbow and wrist.\n",
    "\n",
    "![Keypoints](https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png)\n",
    "* Image Source: https://google.github.io/mediapipe/solutions/pose.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are concerned with only three keypoints that are shoulder, elbow and wrist of the same hand; be it either left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of landmarks = 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[x: 0.4917893409729004\n",
       " y: 0.5736778974533081\n",
       " z: -0.6234160661697388\n",
       " visibility: 0.9998226165771484,\n",
       " x: 0.5185073018074036\n",
       " y: 0.5336750745773315\n",
       " z: -0.5804299712181091\n",
       " visibility: 0.9996976852416992]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total number of landmarks = {}\".format(len(results.pose_landmarks.landmark)))\n",
    "results.pose_landmarks.landmark[0:2] # Output of first two landmarks in the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PoseLandmark.NOSE\n",
      "1 PoseLandmark.LEFT_EYE_INNER\n",
      "2 PoseLandmark.LEFT_EYE\n",
      "3 PoseLandmark.LEFT_EYE_OUTER\n",
      "4 PoseLandmark.RIGHT_EYE_INNER\n",
      "5 PoseLandmark.RIGHT_EYE\n",
      "6 PoseLandmark.RIGHT_EYE_OUTER\n",
      "7 PoseLandmark.LEFT_EAR\n",
      "8 PoseLandmark.RIGHT_EAR\n",
      "9 PoseLandmark.MOUTH_LEFT\n",
      "10 PoseLandmark.MOUTH_RIGHT\n",
      "11 PoseLandmark.LEFT_SHOULDER\n",
      "12 PoseLandmark.RIGHT_SHOULDER\n",
      "13 PoseLandmark.LEFT_ELBOW\n",
      "14 PoseLandmark.RIGHT_ELBOW\n",
      "15 PoseLandmark.LEFT_WRIST\n",
      "16 PoseLandmark.RIGHT_WRIST\n",
      "17 PoseLandmark.LEFT_PINKY\n",
      "18 PoseLandmark.RIGHT_PINKY\n",
      "19 PoseLandmark.LEFT_INDEX\n",
      "20 PoseLandmark.RIGHT_INDEX\n",
      "21 PoseLandmark.LEFT_THUMB\n",
      "22 PoseLandmark.RIGHT_THUMB\n",
      "23 PoseLandmark.LEFT_HIP\n",
      "24 PoseLandmark.RIGHT_HIP\n",
      "25 PoseLandmark.LEFT_KNEE\n",
      "26 PoseLandmark.RIGHT_KNEE\n",
      "27 PoseLandmark.LEFT_ANKLE\n",
      "28 PoseLandmark.RIGHT_ANKLE\n",
      "29 PoseLandmark.LEFT_HEEL\n",
      "30 PoseLandmark.RIGHT_HEEL\n",
      "31 PoseLandmark.LEFT_FOOT_INDEX\n",
      "32 PoseLandmark.RIGHT_FOOT_INDEX\n"
     ]
    }
   ],
   "source": [
    "# Sequential mapping of the landmarks with human body parts\n",
    "\n",
    "count = 0\n",
    "for i in mp_pose.PoseLandmark:\n",
    "    print(count, i)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(a,b,c): # 3D points\n",
    "    ''' Arguments:\n",
    "        a,b,c -- Values (x,y,z, visibility) of the three points a, b and c which will be used to calculate the\n",
    "                vectors ab and bc where 'b' will be 'elbow', 'a' will be shoulder and 'c' will be wrist.\n",
    "        \n",
    "        Returns:\n",
    "        theta : Angle in degress between the lines joined by coordinates (a,b) and (b,c)\n",
    "    '''\n",
    "    a = np.array([a.x, a.y])#, a.z])    # Reduce 3D point to 2D\n",
    "    b = np.array([b.x, b.y])#, b.z])    # Reduce 3D point to 2D\n",
    "    c = np.array([c.x, c.y])#, c.z])    # Reduce 3D point to 2D\n",
    "\n",
    "    ab = np.subtract(a, b)\n",
    "    bc = np.subtract(b, c)\n",
    "    \n",
    "    theta = np.arccos(np.dot(ab, bc) / np.multiply(np.linalg.norm(ab), np.linalg.norm(bc)))     # A.B = |A||B|cos(x) where x is the angle b/w A and B\n",
    "    theta = 180 - 180 * theta / 3.14    # Convert radians to degrees\n",
    "    return np.round(theta, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = None     # Flag which stores hand position(Either UP or DOWN)\n",
    "count = 0       # Storage for count of bicep curls\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.7, min_tracking_confidence=0.5) # Lnadmark detection model instance\n",
    "while cap.isOpened():\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      # Convert BGR frame to RGB\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Make Detections\n",
    "    results = pose.process(image)                       # Get landmarks of the object in frame from the model\n",
    "\n",
    "    # Back to BGR\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)      # Convert RGB back to BGR\n",
    "\n",
    "    try:\n",
    "        # Extract Landmarks\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "\n",
    "        # Calculate angle\n",
    "        angle = calc_angle(shoulder, elbow, wrist)      #  Get angle \n",
    "\n",
    "\n",
    "        # Visualize angle\n",
    "        cv2.putText(image,\\\n",
    "                str(angle), \\\n",
    "                    tuple(np.multiply([elbow.x, elbow.y], [640,480]).astype(int)),\\\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255),2,cv2.LINE_AA)\n",
    "    \n",
    "        # Counter \n",
    "        if angle > 160:\n",
    "            flag = 'down'\n",
    "        if angle < 40 and flag=='down':\n",
    "            count += 1\n",
    "            flag = 'up'\n",
    "        \n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Setup Status Box\n",
    "    cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "    cv2.putText(image, str(count), (10,60), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Render Detections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "\n",
    "    cv2.imshow('MediaPipe feed', image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. Pose Classification [https://google.github.io/mediapipe/solutions/pose_classification.html]\n",
    "\n",
    "2. MediaPipePoseEstimation [https://github.com/nicknochnack/MediaPipePoseEstimation]\n",
    "\n",
    "3. Guide to Human Pose Estimation with Deep Learning[https://nanonets.com/blog/human-pose-estimation-2d-guide/]\n",
    "\n",
    "4. Real-time Human Pose Estimation in the Browser [https://blog.tensorflow.org/2018/05/real-time-human-pose-estimation-in.html]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "314b2b212b00368ea6d1c6877b1dbc2a73dd90aa9dffa5daa0180d41c15719c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('PoseDetection': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
